---
title: "code2vec"
emoji: "📚"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

自身の研究でプログラミングコードをベクトル化する際の技術について調べていたら，[code2vec](https://arxiv.org/abs/1803.09473) に出会ったので，今回はその手法について具体的にまとめたものを自分の備忘録として残しておきます。

## TL;DR

- コードスニペット（プログラミングコード）を抽象構文木（AST）に変換して，その構造をベクトル化
- ベクトル化したものをアテンション・メカニズムというものを使って，コードスニペット全体を表す単一のコードベクトルに変換
- 変換されたコードベクトルは，メソッド名などのコード特性を予測するために使用される

## 理論の部分

理論の部分に関しては[こちら](https://qiita.com/m3yrin/items/b7b11ec2c7d045d07efb)の記事に詳しく書かれています！
「イメージだけ掴みたいよ〜」という方は次の章に進んでください 🚀
https://qiita.com/m3yrin/items/b7b11ec2c7d045d07efb#tldr

## 実装の流れ ~ イメージ ~

今回は下記の関数名 f のコードスニペットを例にして，code2vec がどのようにしてメソッド名を予測しているのかを見ていきましょう！❤️‍🔥

```java
boolean f(Object target) {
    for (Object elem: this.elements) {
        if (elem.equals(target)) {
            return true;
        }
    }
    return false;
}
```

### 1. 抽象構文木（AST）の生成

code2vec は，コードスニペットから 抽象構文木（AST）を作るために**Parser**を使用します．

[IT 辞書](https://e-words.jp/w/%E3%83%91%E3%83%BC%E3%82%B5.html)によると，Parser（パーサ）とは以下のように定義されています．

> パーサ（parser）とは、コンピュータプログラムのソースコードや XML 文書など、何らかの言語で記述された構造的な文字データを解析し、プログラムで扱えるようなデータ構造の集合体に変換するプログラムのこと。そのような処理のことを「構文解析」「パース」(parse)という。

今回の場合の Parser は，
**Java のコードスニペットを解析して，抽象構文木（AST）というデータ構造に変換している**
ということですね！

変換されてできた抽象構文木（AST）は下記のようになります 👇
![](/images/2024-12-30-code2vec/ast_example.png)
_図１：抽象構文木（AST）_

:::message
[こちら](https://code2vec.org/)から AST 図を作成可能
:::

### 2. パスコンテキストの抽出

次に先ほど生成した抽象構文木（AST）から**パスコンテキスト**を抽出します．
下記が論文に記載されているパスコンテキストの定義です．なんのこっちゃ分かりませんね... 😅

> **Definition** **(Path-context)**
> Given an AST Path $p$, its path-context is a triplet <$x_s$, $p$, $x_t$> where $x_s = \phi(start(p))$ and $x_t = \phi(end(p))$ are the values associated with the start and end terminals of $p$.

この定義で言いたいこととしては，AST ノード間のパスを取り出して，それぞれを **「始点 - 経路（始点と終点の間の部分） - 終点」** として表現しているということです．

なんのこっちゃ分からないので，具体例で考えていきましょう！

$x = 7$の場合の表現は下記の通り 👇
$$<x, (NameExpr ↑ AssignExpr ↓ IntegerLiteralExpr), 7>$$

詳しく見ていきましょう 👀

1. 開始ノード(x):

   - AST における始点

2. 経路(p):

   - AST における始点と終点の間の部分
     - `NameExpr` から `AssignExpr` への経路
     - `AssignExpr` から `IntegerLiteralExpr` への経路
   - 矢印は AST 内での移動方向を示す
     - ↑: 親ノードへ
     - ↓: 子ノードへ

3. 終点(y):
   - AST における終点

**経路(p)の AST 構造**

```plaintext
AssignExpr
   /      \
NameExpr   IntegerLiteralExpr
```

- `AssignExpr`: 代入式全体を表す
- `NameExpr`: 左辺の変数 `x` を表す
- `IntegerLiteralExpr`: 右辺のリテラル値 `7` を表す

では，今回の例である関数名 f のコードスニペットに対してパスコンテキストはどのようになるか見ていきましょう！ 🔎

赤いパスに注目すると，パスコンテキストは下記のようになります 👇

$$<elements, (FieldAccessExpr ↑ ForeachStmt ↓ BlockStmt ↓ IfStmt ↓ BlockStmt ↓ ReturnStmt), true>$$

### 3. 埋め込み(Embedding)の計算

次に，各パスコンテキストを固定長のベクトルに変換していきます．

:::details 固定長のベクトルとは？
Gemini に聞いてみました！

> 固定長のベクトルとは、**要素数が事前に決まっていて、変更できないベクトルのこと**です。例えば、3 次元空間の座標を表すベクトル (x, y, z) は、常に 3 つの要素を持ちます。これが固定長のベクトルです。一方、要素数が可変であるベクトルは可変長ベクトルと呼ばれます。例えば、文章を単語に分割したものをベクトルで表す場合、文章の長さによってベクトルの要素数（単語数）が変わります。

> 固定長ベクトルにする理由は、主に以下の点が挙げられます。
>
> - **計算効率の向上**: 固定長のベクトルは、メモリ上に連続して格納されることが多いため、アクセスや計算が効率的に行えます。可変長ベクトルでは、要素数が変わるたびにメモリの再割り当てなどが発生し、オーバーヘッドが生じる可能性があります。
> - **モデルの簡素化**: 機械学習などでは、入力データの形式を固定することで、モデルの構造を簡素化し、学習を安定させられます。例えば、画像認識では、画像を固定サイズのベクトルに変換してから入力することが一般的です。
> - **互換性の確保**: データ交換やシステム連携を行う際、ベクトルの形式を固定することで、互換性を確保しやすくなります。

> 具体的な例として、以下のようなケースで固定長ベクトルが用いられます。
>
> - **画像処理**: 画像を固定サイズにリサイズし、各ピクセルの色情報を要素とするベクトルで表現する。
> - **自然言語処理**: 文章を単語に分割し、各単語に対応するベクトルを固定長に揃える（例えば、最大単語数 100 語までとし、足りない場合はパディングで埋める）。
> - **音声認識**: 音声データを固定長のフレームに分割し、各フレームの特徴量を要素とするベクトルで表現する。

固定長ベクトルは、データの表現方法を統一し、処理を効率化するために重要な役割を果たしているんですね！

:::

埋め込み(Embedding)とはまずなんなのか... 🤔
[AWS のドキュメント](https://aws.amazon.com/jp/what-is/embeddings-in-machine-learning/)がわかりやすそうだったので，記載しておきます．

> 埋め込みとは、機械学習 (ML) や人工知能 (AI) システムが人間のように複雑な知識領域を理解するために使用する、$\textcolor{red}{現実世界のオブジェクトを数値で表現したもの}$です。一例として、計算アルゴリズムは、2 と 3 の差は 1 であることを理解しています。これは、2 と 100 と比較して、2 と 3 の間に密接な関係があることを示しています。ただし、現実世界のデータにはより複雑な関係が含まれています。たとえば、鳥の巣とライオンの巣は似たような組み合わせですが、昼夜は逆の用語です。埋め込みは、**現実世界のオブジェクトを、実世界のデータ間の固有のプロパティと関係をキャプチャする複雑な数学的表現に変換**します。プロセス全体が自動化され、AI システムはトレーニング中に埋め込みを自己作成し、必要に応じてそれを使用して新しいタスクを完了します。

埋め込みを使うことによって，AI が世の中のことをもっと賢く理解することができるようになるんですね！🧠

### コンテキストベクトルの生成

各要素（始点，経路，終点）に対応する埋め込みベクトルを計算して，それらを結合してコンテキストベクトルを生成します．

下記が論文に記載されているコンテキストベクトルの定義です．
$$c_i = embedding(<x_s, p_j, x_t>) = [value\_vocabs_s; path\_vocab_j; value\_vocav_t]$$
各要素（始点，経路，終点）に対応する埋め込みベクトルを計算して，それらを$c_i$（コンテキストベクトル）として表現していますね．

:::message
ここで，$value\_vocab$ は終端（始点，終点）の埋め込みベクトル，$path\_vocab$ は経路の埋め込みベクトルを格納している行列（Matrix）です．
:::

:::details value_vocab と path_vocab についてさらに詳しく 🇬🇧

- **value_vocab**: This matrix stores embeddings for each unique terminal value (tokens) observed during training, such as variable names, literals, and keywords.
- **path_vocab**: This matrix stores embeddings for each unique AST path observed during training.

:::

では，先ほど求めた下記のパスコンテキストのコンテキストベクトルを計算してみましょう！ 🔎

$$<elements, (FieldAccessExpr ↑ ForeachStmt ↓ BlockStmt ↓ IfStmt ↓ BlockStmt ↓ ReturnStmt), true>$$

- $embeddings(elements) = [0.23, -0.45, 0.12, ...]$
- $embeddings(FieldAccessExpr ↑ ForeachStmt ↓ BlockStmt ↓ IfStmt ↓ BlockStmt ↓ ReturnStmt) = [0.67, 0.89, -0.34, ...]$
- $embeddings(true) = [0.45, -0.67, 0.89, ...]$

これらを結合し，1 つのコンテキストベクトルを生成します．

:::message 　 alert
上記の埋め込みは，例なので実際の値とは異なります．
:::

### 4. Attention Mechanism によるパスの重み付け

コードスニペットには複数のパスコンテキストがあるので，Attention Mechanism を使って，これらを単一のコードベクトルに変換します．
Attention Mechanism は，各パスコンテキストにその重要性を反映する重みを割り当てることができます．重みが大きいほど，そのパスコンテキストが重要であるということですね！

下記が論文に記載されている Attention Mechanism の定義です．なんか難しそう... 😅

> Given the combined context vectors: $\{\tilde{c_1}, ..., \tilde{c_n}\}$, the attention weight $\alpha_i$ of each $\tilde{c_i}$ is computed as the normalized inner product between the combined context vector and the global attention vector $a$.

$$attention \ weight \  \alpha_i = \frac{exp(\tilde{c_i}^T \cdot a)}{\sum_{j=1}^{n} exp(\tilde{c_j}^T \cdot a)}$$

$attention \ weight$ は，各パスコンテキストベクトル $\tilde{c_i}^T$ と，グローバルアテンションベクトル $a$ の内積を計算して，それをソフトマックス関数を用いて正規化し，合計が１になるようにしています．

:::details ソフトマックス関数とは？
こちらの記事がわかりやすかったです ❤️
https://antimath-ai.hatenablog.com/entry/sigmoid-and-softmax
:::

では，今回の例の場合，Attention Mechanism によって各パスコンテキストの重みはどのようになるのかをみていきましょう！ 🔎

論文によると，パスの幅がとスコアは比例するとのことなので，今回の例だと $\textcolor{red}{赤}$＞$\textcolor{blue}{青}$＞$\textcolor{green}{緑}$＞$\textcolor{yellow}{黄}$（黄色） の順で重要度が高くなりそうですね．

> The widths of the paths are proportional to the attention score that each of these path-contexts was given.

## なぜこちらの手法が有用なのか？

- 従来のトークンベースの手法では，コードスニペット（プログラミングコード）の構造を考慮できない
  👉 code2vec で提案されている手法を使うことで，コードスニペットが表している内容をコンピュータが理解しやすくなった

## 参考文献

https://arxiv.org/abs/1803.09473
https://e-words.jp/w/%E3%83%91%E3%83%BC%E3%82%B5.html
https://code2vec.org/
https://aws.amazon.com/jp/what-is/embeddings-in-machine-learning/
